"""
S&P500 + NASDAQ100 ì¢…ëª© í•œê¸€ëª… ìë™ ìˆ˜ì§‘ ìŠ¤í¬ë¦½íŠ¸

ì„¤ê³„ ì˜ë„:
- Wikipedia í•œêµ­ì–´íŒì—ì„œ ê¸°ì—… í•œê¸€ëª… ìë™ ìˆ˜ì§‘
- Wikipedia APIë¥¼ ì‚¬ìš©í•˜ì—¬ í‹°ì»¤ â†’ í•œê¸€ëª… ë§¤í•‘
- ê²°ê³¼ë¥¼ JSONê³¼ TypeScript íŒŒì¼ë¡œ ì €ì¥
- BuffettCard ì»´í¬ë„ŒíŠ¸ì—ì„œ ì‚¬ìš©í•  í•œê¸€ëª… ë°ì´í„° ìƒì„±

ì‚¬ìš©ë²•:
  python fetch_korean_names.py
"""

import json
import time
import requests
import pandas as pd
from pathlib import Path
from typing import Optional

# ============================================================
# ì„¤ì •
# ============================================================
WIKIPEDIA_API = "https://ko.wikipedia.org/w/api.php"
HEADERS = {
    "User-Agent": "BitcosKoreanNameFetcher/1.0 (https://bitcos.io; contact@bitcos.io)"
}

# ì´ë¯¸ ì•Œë ¤ì§„ í•œê¸€ëª… (Wikipediaì—ì„œ ì°¾ê¸° ì–´ë ¤ìš´ ê²½ìš° ìˆ˜ë™ ë§¤í•‘)
KNOWN_KOREAN_NAMES = {
    # ë¹…í…Œí¬
    "AAPL": "ì• í”Œ",
    "MSFT": "ë§ˆì´í¬ë¡œì†Œí”„íŠ¸",
    "GOOGL": "ì•ŒíŒŒë²³",
    "GOOG": "ì•ŒíŒŒë²³",
    "AMZN": "ì•„ë§ˆì¡´",
    "META": "ë©”íƒ€",
    "NVDA": "ì—”ë¹„ë””ì•„",
    "TSLA": "í…ŒìŠ¬ë¼",
    # ë°˜ë„ì²´
    "AMD": "AMD",
    "INTC": "ì¸í…”",
    "AVGO": "ë¸Œë¡œë“œì»´",
    "QCOM": "í€„ì»´",
    "TXN": "í…ì‚¬ìŠ¤ì¸ìŠ¤íŠ¸ë£¨ë¨¼íŠ¸",
    "MU": "ë§ˆì´í¬ë¡ ",
    "ASML": "ASML",
    "ARM": "ARM",
    # ê¸ˆìœµ
    "JPM": "JPëª¨ê±´",
    "BAC": "ë±…í¬ì˜¤ë¸Œì•„ë©”ë¦¬ì¹´",
    "WFC": "ì›°ìŠ¤íŒŒê³ ",
    "C": "ì‹œí‹°ê·¸ë£¹",
    "GS": "ê³¨ë“œë§Œì‚­ìŠ¤",
    "MS": "ëª¨ê±´ìŠ¤íƒ ë¦¬",
    "V": "ë¹„ì",
    "MA": "ë§ˆìŠ¤í„°ì¹´ë“œ",
    "AXP": "ì•„ë©”ë¦¬ì¹¸ìµìŠ¤í”„ë ˆìŠ¤",
    "BRK-B": "ë²„í¬ì…”í•´ì„œì›¨ì´",
    # ì†Œë¹„ì¬
    "KO": "ì½”ì¹´ì½œë¼",
    "PEP": "í©ì‹œì½”",
    "PG": "P&G",
    "WMT": "ì›”ë§ˆíŠ¸",
    "COST": "ì½”ìŠ¤íŠ¸ì½”",
    "MCD": "ë§¥ë„ë‚ ë“œ",
    "NKE": "ë‚˜ì´í‚¤",
    "SBUX": "ìŠ¤íƒ€ë²…ìŠ¤",
    "HD": "í™ˆë””í¬",
    # í—¬ìŠ¤ì¼€ì–´
    "JNJ": "ì¡´ìŠ¨ì•¤ì¡´ìŠ¨",
    "UNH": "ìœ ë‚˜ì´í‹°ë“œí—¬ìŠ¤",
    "PFE": "í™”ì´ì",
    "MRK": "ë¨¸í¬",
    "ABBV": "ì• ë¸Œë¹„",
    "LLY": "ì¼ë¼ì´ë¦´ë¦¬",
    # í†µì‹ /ë¯¸ë””ì–´
    "VZ": "ë²„ë¼ì´ì¦Œ",
    "T": "AT&T",
    "NFLX": "ë„·í”Œë¦­ìŠ¤",
    "DIS": "ë””ì¦ˆë‹ˆ",
    # ì—ë„ˆì§€
    "XOM": "ì—‘ìŠ¨ëª¨ë¹Œ",
    "CVX": "ì…°ë¸Œë¡ ",
    # ì‚°ì—…ì¬
    "BA": "ë³´ì‰",
    "CAT": "ìºí„°í•„ëŸ¬",
    "GE": "GEì—ì–´ë¡œìŠ¤í˜ì´ìŠ¤",
    "HON": "í•˜ë‹ˆì›°",
    "UPS": "UPS",
}


def get_sp500_tickers() -> list[str]:
    """
    S&P 500 í‹°ì»¤ ë° íšŒì‚¬ëª… ë¦¬ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
    
    Returns:
        list: [(ticker, company_name), ...]
    """
    try:
        print("\nğŸ” S&P 500 í‹°ì»¤ ë¦¬ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ëŠ” ì¤‘...")
        url = "https://raw.githubusercontent.com/datasets/s-and-p-500-companies/master/data/constituents.csv"
        df = pd.read_csv(url)
        
        # (ticker, company_name) íŠœí”Œ ë¦¬ìŠ¤íŠ¸
        tickers = []
        for _, row in df.iterrows():
            ticker = str(row["Symbol"]).strip().replace(".", "-")
            name = str(row["Name"]).strip() if "Name" in df.columns else ""
            tickers.append((ticker, name))
        
        print(f"âœ… S&P 500: {len(tickers)}ê°œ ì¢…ëª©")
        return tickers
    except Exception as e:
        print(f"âŒ S&P 500 ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
        return []


def get_nasdaq100_tickers() -> list[str]:
    """
    NASDAQ 100 í‹°ì»¤ ë° íšŒì‚¬ëª… ë¦¬ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
    
    Returns:
        list: [(ticker, company_name), ...]
    """
    try:
        print("\nğŸ” NASDAQ 100 í‹°ì»¤ ë¦¬ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ëŠ” ì¤‘...")
        url = "https://en.wikipedia.org/wiki/Nasdaq-100"
        headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"}
        
        tables = pd.read_html(requests.get(url, headers=headers).content)
        
        nasdaq100_df = None
        for table in tables:
            if "Ticker" in table.columns or "Symbol" in table.columns:
                nasdaq100_df = table
                break
        
        if nasdaq100_df is None:
            print("âŒ NASDAQ 100 í…Œì´ë¸”ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            return []
        
        ticker_col = "Ticker" if "Ticker" in nasdaq100_df.columns else "Symbol"
        name_col = "Company" if "Company" in nasdaq100_df.columns else None
        
        tickers = []
        for _, row in nasdaq100_df.iterrows():
            ticker = str(row[ticker_col]).strip()
            name = str(row[name_col]).strip() if name_col else ""
            tickers.append((ticker, name))
        
        print(f"âœ… NASDAQ 100: {len(tickers)}ê°œ ì¢…ëª©")
        return tickers
    except Exception as e:
        print(f"âŒ NASDAQ 100 ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
        return []


def search_korean_wikipedia(company_name: str, ticker: str) -> Optional[str]:
    """
    Wikipedia í•œêµ­ì–´íŒì—ì„œ íšŒì‚¬ í•œê¸€ëª… ê²€ìƒ‰
    
    Args:
        company_name: ì˜ë¬¸ íšŒì‚¬ëª…
        ticker: í‹°ì»¤ ì‹¬ë³¼
    
    Returns:
        í•œê¸€ íšŒì‚¬ëª… ë˜ëŠ” None
    """
    # 1. ì´ë¯¸ ì•Œë ¤ì§„ í•œê¸€ëª…ì´ ìˆìœ¼ë©´ ë°˜í™˜
    if ticker in KNOWN_KOREAN_NAMES:
        return KNOWN_KOREAN_NAMES[ticker]
    
    # 2. Wikipedia APIë¡œ ê²€ìƒ‰
    search_terms = [
        company_name,
        f"{company_name} ê¸°ì—…",
        ticker,
    ]
    
    for search_term in search_terms:
        try:
            # Wikipedia ê²€ìƒ‰ API
            params = {
                "action": "query",
                "list": "search",
                "srsearch": search_term,
                "format": "json",
                "srlimit": 3,
            }
            
            response = requests.get(WIKIPEDIA_API, params=params, headers=HEADERS, timeout=10)
            data = response.json()
            
            if "query" in data and data["query"]["search"]:
                # ì²« ë²ˆì§¸ ê²€ìƒ‰ ê²°ê³¼ì˜ ì œëª© ë°˜í™˜
                title = data["query"]["search"][0]["title"]
                
                # íšŒì‚¬ëª… ê´€ë ¨ ê²°ê³¼ì¸ì§€ í™•ì¸ (ê¸°ì—…, íšŒì‚¬, Inc, Corp ë“±)
                if any(keyword in title for keyword in ["ê¸°ì—…", "íšŒì‚¬", company_name.split()[0]]):
                    return title
            
            time.sleep(0.3)  # API ìš”ì²­ ê°„ê²©
            
        except Exception:
            continue
    
    return None


def extract_korean_name(wiki_title: str, company_name: str) -> str:
    """
    Wikipedia ì œëª©ì—ì„œ ìˆœìˆ˜ í•œê¸€ëª… ì¶”ì¶œ
    
    Args:
        wiki_title: Wikipedia ë¬¸ì„œ ì œëª©
        company_name: ì˜ë¬¸ íšŒì‚¬ëª… (ë¹„êµìš©)
    
    Returns:
        ì •ì œëœ í•œê¸€ëª…
    """
    # "(ê¸°ì—…)", "(íšŒì‚¬)" ë“± ì œê±°
    import re
    clean_name = re.sub(r'\s*\([^)]*\)', '', wiki_title).strip()
    
    # ë„ˆë¬´ ê¸¸ë©´ ì²« ë‹¨ì–´ë§Œ
    if len(clean_name) > 15:
        clean_name = clean_name.split()[0]
    
    return clean_name


def fetch_all_korean_names() -> dict[str, str]:
    """
    ëª¨ë“  S&P500 + NASDAQ100 ì¢…ëª©ì˜ í•œê¸€ëª… ìˆ˜ì§‘
    
    Returns:
        dict: {ticker: korean_name}
    """
    print("\n" + "=" * 80)
    print("ğŸš€ ë¯¸êµ­ ì£¼ì‹ í•œê¸€ëª… ìˆ˜ì§‘ ì‹œì‘")
    print("=" * 80)
    
    # í‹°ì»¤ ìˆ˜ì§‘
    sp500 = get_sp500_tickers()
    nasdaq100 = get_nasdaq100_tickers()
    
    # ì¤‘ë³µ ì œê±°
    all_stocks = {}
    for ticker, name in sp500 + nasdaq100:
        if ticker not in all_stocks:
            all_stocks[ticker] = name
    
    print(f"\nğŸ“Š ì´ {len(all_stocks)}ê°œ ì¢…ëª© ì²˜ë¦¬ ì˜ˆì •")
    
    # í•œê¸€ëª… ìˆ˜ì§‘
    korean_names = {}
    success_count = 0
    
    for i, (ticker, company_name) in enumerate(all_stocks.items()):
        print(f"\râ³ ì§„í–‰ ì¤‘: {i+1}/{len(all_stocks)} ({ticker})...", end="", flush=True)
        
        korean_name = search_korean_wikipedia(company_name, ticker)
        
        if korean_name:
            clean_name = extract_korean_name(korean_name, company_name)
            korean_names[ticker] = clean_name
            success_count += 1
        
        # API ìš”ì²­ ê°„ê²© (Wikipedia ì •ì±… ì¤€ìˆ˜)
        time.sleep(0.5)
    
    print(f"\n\nâœ… í•œê¸€ëª… ìˆ˜ì§‘ ì™„ë£Œ: {success_count}/{len(all_stocks)}ê°œ ì„±ê³µ")
    
    return korean_names


def save_results(korean_names: dict[str, str]):
    """
    ê²°ê³¼ë¥¼ JSONê³¼ TypeScript íŒŒì¼ë¡œ ì €ì¥
    
    Args:
        korean_names: {ticker: korean_name}
    """
    output_dir = Path(__file__).parent.parent.parent / "src" / "lib" / "data"
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # 1. JSON íŒŒì¼
    json_path = output_dir / "korean-stock-names.json"
    with open(json_path, "w", encoding="utf-8") as f:
        json.dump(korean_names, f, ensure_ascii=False, indent=2)
    print(f"\nğŸ’¾ JSON ì €ì¥: {json_path}")
    
    # 2. TypeScript íŒŒì¼
    ts_path = output_dir / "korean-stock-names.ts"
    ts_content = '''/**
 * S&P500 + NASDAQ100 ì¢…ëª© í•œê¸€ëª… ë§¤í•‘
 * 
 * ìƒì„±: fetch_korean_names.py ìŠ¤í¬ë¦½íŠ¸
 * ì—…ë°ì´íŠ¸: í•„ìš”ì‹œ ìŠ¤í¬ë¦½íŠ¸ ì¬ì‹¤í–‰
 */

export const KOREAN_STOCK_NAMES: Record<string, string> = {
'''
    
    # ì•ŒíŒŒë²³ ìˆœ ì •ë ¬
    for ticker in sorted(korean_names.keys()):
        name = korean_names[ticker]
        ts_content += f'  "{ticker}": "{name}",\n'
    
    ts_content += "};\n"
    
    with open(ts_path, "w", encoding="utf-8") as f:
        f.write(ts_content)
    print(f"ğŸ’¾ TypeScript ì €ì¥: {ts_path}")


def main():
    """ë©”ì¸ ì‹¤í–‰"""
    korean_names = fetch_all_korean_names()
    
    if korean_names:
        save_results(korean_names)
        
        print("\n" + "=" * 80)
        print("âœ… ì™„ë£Œ!")
        print("=" * 80)
        print("\në‹¤ìŒ ë‹¨ê³„:")
        print("1. src/lib/data/korean-stock-names.ts íŒŒì¼ í™•ì¸")
        print("2. BuffettCard.tsxì—ì„œ importí•´ì„œ ì‚¬ìš©")
        print('   import { KOREAN_STOCK_NAMES } from "@/lib/data/korean-stock-names";')
    else:
        print("\nâŒ í•œê¸€ëª… ìˆ˜ì§‘ ì‹¤íŒ¨")


if __name__ == "__main__":
    main()
